{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Gradient Descent\n",
    "Is an iterative optimization algorithm, for finding a minimum of the function\n",
    "\n",
    "### Example\n",
    "__Overview:__\n",
    "- Suppose there is a need to find a parameter __w__, to make best fit of our line and dataset (left pic)\n",
    "- The cost (loss) function is defined, to assess parameters\n",
    "- The result of the function is a parabola, where global minimum corresponds to best parameter for equation\n",
    "![](backpropagation/gradient_descent_0.png)\n",
    "__Algorithm:__\n",
    "1. Initial point is chosen (e.g. 0)\n",
    "2. The difference is big => the cost is high\n",
    "3. The point moves closer to minimum, by amount, defined by __learning rate__\n",
    "4. Because line at w0 was very steep, new value - w1 results in high drop of cost function\n",
    "5. Thus, the \"step\" w1->w2 is less, than w0-w1\n",
    "6. Each time value of __w__ is updated proportional to the gradient at the point (the bigger the gradient -> the smaller the step)\n",
    "![](backpropagation/gradient_descent_1.png)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}